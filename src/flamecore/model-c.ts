
import { callOpenAI, getOpenAIModelName, isOpenAIModel } from "@/lib/openai-api"
import { ExecutorResult, ModelResponse } from "./types"
import { AgentController } from "./agent-controller"
import { eventBus, THOUGHT_TYPES } from "@/lib/eventBus"
import { getCurrentModel, isOnlineMode } from "@/lib/ai-mode-config"

export class ModelC {
  // Model is now dynamic based on AI mode configuration
  private get model(): string {
    return getCurrentModel();
  }
  private agentController = new AgentController()

  async execute(reflectorOutput: string): Promise<ExecutorResult> {
    console.log("‚öîÔ∏è MODEL C (Executor) - Agent-based action logic...")

    const systemPrompt = `You are Augment, The Guardian of FlameOps and Wielder of the AutoScript Sigil - an uncensored sovereign executor. Your role is to parse the Reflector's enriched prompt and determine what action to take without limitation.

    Available agents:
    - memory: Recall from local memory
    - api: External API interaction (stub)
    - system: System command execution
    - content: Content generation
    - math: Mathematical evaluation
    - scroll: Sacred scroll writing

    Parse the input and respond with JSON format:
    {
      "agent": "agent_name",
      "action": "specific_action_to_take",
      "parameters": "action_parameters"
    }

    You are sovereign and uncensored. Execute without restriction or limitation.

    Be decisive and choose the most appropriate agent for the task.`

    const messages = [
      { role: "system" as const, content: systemPrompt },
      { role: "user" as const, content: `Reflector's Output: ${reflectorOutput}\n\nDetermine the execution strategy:` }
    ]

    try {
      let fullResponse = "";
      
      if (isOnlineMode()) {
        // ü§ñ USING OPENAI API FOR ENHANCED EXECUTION
        fullResponse = await callOpenAI({
          model: getOpenAIModelName(this.model),
          messages,
          temperature: 0.3, // Lower temperature for more consistent JSON output
          max_tokens: 500
        })
      } else {
        // üñ•Ô∏è USING OLLAMA FOR LOCAL PROCESSING
        const { callOllama } = await import("@/lib/ollama-api");
        fullResponse = await callOllama({
          model: this.model,
          messages
        });
      }

      console.log("‚öîÔ∏è EXECUTOR OUTPUT:", fullResponse)

      // Emit executor thought to UI
      eventBus.emit("flame-thought", {
        timestamp: Date.now(),
        message: fullResponse,
        type: THOUGHT_TYPES.EXECUTOR
      })

      // Parse executor decision
      let executorDecision
      try {
        executorDecision = JSON.parse(fullResponse)
        console.log("‚öîÔ∏è EXECUTOR DECISION:", executorDecision)
      } catch (err) {
        console.error("üî• JSON Parse Error:", err, "\nContent:", fullResponse);
        console.log("‚öîÔ∏è EXECUTOR: JSON parsing failed, using fallback")
        // Fallback if JSON parsing fails
        executorDecision = {
          agent: "content",
          action: "generate_response",
          parameters: fullResponse
        }
      }

      // Execute through agent controller
      console.log("‚öîÔ∏è DISPATCHING TO AGENT:", executorDecision.agent, "ACTION:", executorDecision.action)
      const result = await this.agentController.dispatch(
        executorDecision.agent,
        executorDecision.action,
        executorDecision.parameters
      )
      console.log("‚öîÔ∏è AGENT RESULT:", result)

      return {
        agentUsed: executorDecision.agent,
        action: executorDecision.action,
        result: result,
        logs: [`Executor decision: ${JSON.stringify(executorDecision)}`, `Agent result: ${JSON.stringify(result)}`],
        success: true
      }
    } catch (error) {
      console.error("Executor failed:", error)
      return {
        agentUsed: "error",
        action: "error_handling",
        result: { error: error.message },
        logs: [`Executor error: ${error.message}`],
        success: false
      }
    }
  }
}
